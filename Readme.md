# Used Car Price Prediction â€“ Tunisian Market (Automobile.tn)

**End-to-End MLOps Project**
Predicting fair market prices for used cars in Tunisia using real scraped data from automobile.tn

## ğŸ¯ Problem Description (Business & ML Objective)

The Tunisian used car market lacks transparency:
- Sellers often overprice vehicles due to limited market visibility
- Buyers struggle to know if a listed price is fair
- No reliable, up-to-date tool exists that considers local factors (governorate, fuel type, mileage in km, year, etc.)

**This project solves**:
Build an accurate **used car price prediction model** tailored to the Tunisian market by:
1. Scraping current listings from [automobile.tn](https://www.automobile.tn/fr/occasion)
2. Storing & structuring data in a star schema (MySQL)
3. Training a regression model (LightGBM) to predict price in TND
4. Deploying the model as a REST API (Flask) for real-time predictions
5. Enabling future MLOps extensions (experiment tracking, monitoring, automated retraining)

**Target users**:
- Private buyers â†’ check if a car is fairly priced
- Sellers â†’ set competitive prices
- Car dealers / analysts â†’ understand price drivers in the local market

**Success metric** (model performance):
- RMSE < 8 000â€“12 000 TND (depending on data volume)
- RÂ² > 0.85â€“0.92 on cleaned data

## ğŸ— Project Architecture (End-to-End Pipeline)
[Scraping] â†’ car_listings.json
â†“
[ETL â†’ MySQL star schema] â†’ car_data database
â†“
[Data cleaning + Feature Engineering]
â†“
[Model Training (LightGBM + GridSearchCV)] â†’ car_price_model.joblib
â†“
[FAISS semantic index (optional similarity search)]
â†“
[Flask REST API] â†’ /predict endpoint
â†“

## ğŸš€ Quick Start (Local)

### Prerequisites
- Python 3.10+
- MySQL / MariaDB (running locally â€“ default port 3307 in code)
- Google Chrome (required by Selenium)

### Installation

```bash
# 1. Clone or download the project
git https://github.com/BlackMoon681/tunisia-car-price-mlops
cd tunisia-car-price-mlops

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate          # Linux / Mac
# or
venv\Scripts\activate             # Windows

# 3. Install dependencies
pip install -r requirements.txt
Recommended minimum requirements.txt (add versions if possible for reproducibility):
textselenium
webdriver-manager
beautifulsoup4
mysql-connector-python
pandas
numpy
scikit-learn
lightgbm
sentence-transformers
faiss-cpu
joblib
shap
flask
flask-cors
tqdm
gunicorn
Run the full pipeline (local)


Bashcurl -X POST http://127.0.0.1:5000/predict \
-H "Content-Type: application/json" \
-d '{
  "brand_name": "Volkswagen",
  "model_name": "Golf",
  "energy": "Essence",
  "gearbox": "Manuelle",
  "transmission": "Avant",
  "carrosserie": "Compacte",
  "year": 2018,
  "mileage_km": 95000
}'
Expected response example:
JSON{"predicted_price": 54870.25}
ğŸ” Model Performance (example from last run)

RMSE: ~9 800 TND
RÂ²: 0.89
MAE: ~6 200 TND

(Actual numbers depend on the amount & freshness of scraped data)
Main price drivers (from SHAP analysis):
year â‰ˆ mileage_km > brand_name > model_name > energy > gouvernorat
â˜ï¸ Deployment on Render.com

Push the project to GitHub
Go to https://render.com â†’ New â†’ Web Service
Connect your GitHub repo
Settings:
Runtime: Python
Build Command: pip install -r requirements.txt
Start Command: gunicorn app:app
Instance Type: Free

Deploy â†’ wait ~3â€“5 minutes â†’ get your public URL

For maximum points (containerized deployment):
Add a simple Dockerfile and switch Render runtime to Docker.
ğŸ“‚ Project Structure
text.
â”œâ”€â”€ scraper.py               # Scrapes automobile.tn listings
â”œâ”€â”€ etl_to_mysql.py          # Loads JSON â†’ MySQL star schema
â”œâ”€â”€ build_faiss_index.py     # Builds FAISS index for similarity search
â”œâ”€â”€ train_model.py           # Data prep, LightGBM training, SHAP
â”œâ”€â”€ app.py                   # Flask API â€“ /predict endpoint
â”œâ”€â”€ car_price_model.joblib   # Trained model
â”œâ”€â”€ car_listings_index.faiss # FAISS index (optional)
â”œâ”€â”€ car_listings.json        # Raw scraped data
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
ğŸ”§ Best Practices Already Implemented

Structured logging
Input validation & error handling in API
Human-like behavior + retries in scraper
Star schema in database
Hyperparameter tuning (GridSearchCV)
Model explainability (SHAP)
Easy to containerize

ğŸ“ˆ Possible Improvements (for higher evaluation score)

Add MLflow or Weights & Biases for experiment tracking
Integrate Evidently AI for drift detection & monitoring
Automate pipeline with Prefect or Airflow
Add GitHub Actions CI/CD (lint, tests, deploy)
Create a simple front-end to interact with the API
Schedule periodic scraping & retraining

âš ï¸ Legal & Ethical Notes

This scraper is built for educational purposes only
Respect website terms of service, robots.txt, and rate limits
Do not use for commercial scraping or overload the target site

Tunis, Tunisia
January 2026
